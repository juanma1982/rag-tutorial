Hola, hoy vamos a ver árboles. Y vamos a comenzar por el primero de ellos, el más sencillo de todos, el E de tres. Bueno, E de tres significa iterative, DicoTomizer 3. Y acá el tres no viene a ser un número de versión, sino un juego de palabras. Si su creador, CrossKineland, utilizó el número tres por su similitud con la palabra 3 en inglés, que significa árbol. Es decir, que el nombre quiere decir DicoTomizer iterativo de árboles. Muy bien, ¿qué es lo que hace el gorismo? Este algoritmo genera un árbol de decisión a partir de un conjunto de ejemplos. Muy bien, ¿qué es una árbol de decisión? Una árbol de decisión es un grafo. Como pueden ver acá en este imagen. Es un grafo que tiene unas características. En principio, que no hay ciclos, es un grafo asíquico. Tiene algunos componentes, como por ejemplo, el nodo principal, el cual llamamos Rise y que está siempre en la parte superior. Por lo menos cuando lo representamos. Luego están los nodos terminales, que son estas cajitas de acá, los nodos terminales indican básicamente que de allí no va a ver ninguna rama. Ahí el árbol termina y nosotros vamos a encontrar en ese punto una respuesta a la pregunta que formulamos para recorrer el árbol. Los otros nodos, estos que están acá, que se representan con óvalo, son preguntas, preguntas que tenemos que responder. Y finalmente, las líneas indican las posibles respuestas. Entonces, una pregunta, por ejemplo, ¿cómo está el clima hoy? ¿Y el clima hoy? ¿Y suvioso, soleado, ventoso? Cada arista es una respuesta y eso nos va a llevar a una nueva pregunta con un nodo terminal. Para construir este árbol a partir de los ejemplos lo que necesitamos conocer es un concepto llamado Entropía de la información. Que para aquellos que hayan hecho físicos, les va a resultar familiar el nombre y este concepto es analogo al concepto de Entropía en física, que es la entropía de la energía. Básicamente, la entropía de la información es una medida del desorden de la información, de los datos. ¿Qué tan aleatorios son esos datos? También es otra forma de tenerlo es una medida de la pureza. Vamos a trabajar un poco la idea de pureza. Y esta es la fórmula. Si queremos calcular la entropía de N-clases se utiliza esta fórmula acá. ¿Qué serían en el clase? Bueno, yo quiero construir el evento en un clasificador. El clasificador va a tener diferentes alias, por ejemplo, haciendo un clasificador de críticas, cinematográficas, las clases pueden hacer positiva, neutra o negativa. Estas donas clases. En este caso, tres. Ahora, para esas clases, ¿qué tan desordenado está el conjunto de ejemplos que yo tengo? Para eso tengo que ver cuál es la probabilidad y, si es cada una de esas clases, y multiplicarla por el hogar y no más cedos de la probabilidad, luego, hago una sumatoria por cada una de ellas y de esa manera voy a obtener la entropía del sistema. En este caso, ese es una lista de valores posibles, pdí es la probabilidad de los valores, eí cada uno de los valores. Muy bien, un par de cosas para tener en cuenta. Si la muestra es homogénea, la entropía es igual a cero. ¿Qué quiere decir que la muestra es homogénea? No básicamente que todos los casos son iguales. Si yo tengo una bolsa, llena de pelotitas blancas, la probabilidad es acá una pelotita negra es cero, que la probabilidad es acá una pelotita blanca es uno. No hay desorden, no hay entropía tampoco. Entonces, la máxima entropía viene dada por logar y no más cedos de N, donde N son los posibles valores de salida esto, el capagodo de Ciel. Ciel es fuesedo, por ejemplo, trubo, folz, o blanco y negro, o positivo negativo con Kiran, verlo, pero tengo dos clases. La máxima entropía es uno, quiere decir que es la máxima incertidumbre. Por ejemplo, si yo en esta bolsa tuviese 50 pelotitas negres, 50 pelotitas blancas, ahora la probabilidad es sacar una pelotita de cualquier polo, va a ser de 0.5. Y esa máxima incertidumbre. No puede haber otro estado en el cual yo tenga mayor incertidumbre sobre el que voy a sacar. Y en ese caso, la entropía va a ser uno. Muy bien, ¿vamos un ejemplo de cómo funciona la entropía para tirar un dado? Todos sabemos que la probabilidad de sacar cualquier número, un un dado es un sexto. Y pensando de lo como un clasificador tendríamos seis clases, ¿no? 6 por sí los resultados, 1, 2, 3, 4, 5, 6. Eso quiere decir que la probabilidad, vamos a manejar acá como un sexto por lo varino más 2, de 6, que es lo mismo que menos lo varino más 2 de un sexto. Y los sumamos a 6 veces, que es lo mismo que multiplicarlo por 6 y nos da una entropía de 2,58. Ese es la entropía que tiene tirar un dado. Otro ejemplo. Sumonamos que tenemos unos mensajes y el número de estados en un mensaje puede ser 3. M1, M2 o M3. Pero la probabilidad de cada mensaje diferente, 50 es el M1, 25 por 100 de los M2 y 25 de la M3. En ese caso, la entropía sería un medio por logaritmo más 2, de 2 más con 4 por logaritmo más 2 de 4 más, un 4 por logaritmo más 2 de 4. Y la entropía para recibir un mensaje igual que la sería 1.5. Un ejemplo más. Fíjense. Tenemos acá un conjunto en donde tenemos elementos que pueden ser negativos o positivos. Y son todos negativos. Esto ya acá. Entropía 0. Si agrego un par de positivos, si tengo que quien ser 5 negativos, 2 positivos, voy a tener una entropía más o menos de 0.7. Cuando la cosa esté cribrada, tengo 3 positivos y 3 negativos. La entropía es máxima 1. Que dijimos que es la entropía máxima para un sistema que tiene 2 clases. Ahora, si la cosa es invierte, empezamos a tener más positivos y menos negativos, la entropía vuelve a bajar. En este caso 0.8, 0.4 acá, donde tenemos solo 2 negativos y el resto positivo. Y es 1 cuando lo perdemos acá, es la probabilidad positiva. Cuando son 2 positivos, sí. Lo perdemos acá, es la probabilidad positiva. La probabilidad positiva puede ser 0, puede ser 1. Y de este lado tenemos la entropía. Muy bien. ¿Para qué explique la entropía para poder presentarles este nuevo concepto que es ganancia de información? Que es el que nos interesa. No es la entropía, sino la ganancia. Y que es la ganancia de la información. Bueno, básicamente es una forma de cuántificar que elemento en un conjunto me está dando más información. Entonces, imagínen que tuvimos un conjunto de datos, puede ser una tabla, un archivo como hace parito de valio, con 10 columnas. Y yo quisiera decir una sola de esas columnas para hacer un clasificador. Posiblemente ninguna sea un clasificador perfecto. Pero posible que haya alguna columna que cometa menos errores al clasificar que las demás. Si yo tuviera que mirar una sola. Bueno, esa columna es la que me da la mayor ganancia de información. Y la ganancia de información se la puede calcular con esta fórmula. La entropía del sistema, que ya dije como calcularla, menos y acá restamos la ganancia de información de el atributo A. ¿Dónde ese va a ser una lista de valores posibles para ese atributo A, uno de varios atributos posibles, acá cuando hay attribute, pueden pensar que es una columna. Una columna, una columna, varias columnas. Y los valores posibles de una tributo son los valores posibles que vamos a encontrar cada uno de los registros de esa tabla o archivo lo que sea. Entonces, BDA son los valores que ya puede tomar. Ese debe sobre... Ese es la probabilidad de un valor para el atributo A. Que está muy probable que ella atributo para escala columna. Y finalmente tenemos la entropía. Ese debe que es la entropía. La columna abrimos antes, pero calculada para el valor B, es un valor solo de la tributo A. Entonces, cuando yo sumo todos los valores del atributo A, a un asumato, esto lo tenemos regular para cada valor del atributo, calculo esta probabilidad, lo multiplico para la entropía del valor y se lo resta a la entropía general. Y eso me va a dar la ganancia de información. Yo con la ganancia lo que voy a querer buscar es ver cuál de todos estos atributos tiene la mayor ganancia. Ahora, que ya saben que es la entropía. Y como calcula la ganancia de información les voy a decir cuál es el algoritmo y de tres. Paso 1, calcula la entropía para todas las clases. Paso 2, calcula la entropía para cada valor posible de cada tributo. Paso 3, selecciona el mejor atributo basado en la reducción de la entropía. Y usa para eso el cálculo de ganancia de la información. Punto 4 y iteramos. Para cada sugnodo, excluyendo el nodorraiz que ya ha fuzado. ¿Qué quiere decir esto? Que ese primer atributo lo vamos a colocar como nodorraiz. Lo quitamos del conjunto de atributos y seguimos iterando. Buscamos el segundo, mejor atributo y va a hacer un hijo de este nodorraiz. Y así, hasta que haya podido clasificar correctamente todos los ejemplos. Para poder entender estos clascagos, explicarles de forma teórica y un poco en el aire, vamos a ver un ejemplo muy muy puntual. En este ejemplo tenemos básicamente seis animales. 6 o 8, 1, 2, 3, 4, 5 o 6, 7 animales. Acá animal viene a ser el ID del atributo. La 10 vaca ha cerrado paloma murciela o gallina y guano o cocodrino. ¿Qué atributos tenemos? Vuela, piel y nacimiento. Vuela puede ser si o no si no atributo o buliano. Si o la vaca vuela no, la paloma vuela sí. Piel de que está cubierto ese animal. Puede ser de pelo, de plumas o de escamas. Y finalmente, ¿cómo nace? Puede ser en un huevo, como por ejemplo un cocodrilo, un iguana o una paloma o pueden hacer en un útero y por eso agarricimos ponemos clasenta. Nacen en un útero, nace en una placenta. Pásicamente vendría a ser un tipo huillano también. No hay dos posibilidades. La centa huevo o útero huevo se prefieren. Y lo que nos interesa a nosotros es circular este atributo, esta clase. Queremos saber si el animal es una vaca, es una vaca, es un mamífero, es un reptil o es una vez. Entonces vamos a hacer el algoritmo de 3, nosotros paso a paso. Muy bien. ¿Cuáles son las probabilidades de estas clases? La probabilidad de mamíferos 3 séptimos, la probabilidad de ave estos séptimos, la probabilidad de las clases 2 séptimos también. ¿Por qué? Porque en este conjunto tengo siete elementos y de los cuales 3 son mamíferos 2 son aves y son reptiles. Hay que aclarar algo acá, esta probabilidad es una probabilidad que le cuida sobre el conjunto de datos. No es la probabilidad real. Si yo hago un animal random, un animal aleatorio, el planeta tierra al azar, la probabilidad que sea un mamífero es nuestra séptima. Ni la probabilidad que sea aves es el séptimo. Por eso es importante siempre entender si nuestro conjunto de datos es representativo del universo o no. Y si no lo es, bueno, para que caso sirve que se os servirá quizás a representativo de una granja, de una zona, de una área rural, lo que sea, o de algún problema que queremos resolver. Pero hablando así, me voy pronto y decir que la probabilidad de una animal sea vaca, digo sea mamíferos 3 séptimos, no sería correcto. Muy bien, esta es la probabilidad y esta es la segunda parte de la ecuación, lo que está dentro de la sumatoria, quieren que sea menos probabilidad por lo que hay que no sea de la probabilidad. Y calculado para cada uno de los valores, da 0.52. Esta parte es y traga. Y yo lo sumo es 1.56. Eso quiere decir, la entropea del conjunto es 1.56. Entonces, ahora vamos a trabajar atributo para atributo. Primero atributo, atributo vuela. Para cada atributo vamos a calcular para cada valor. Atributo vuela igual no. ¿Qué quiere decir esto? Vamos a tachar entre comillas los atributos que no corresponden a vuela igual no. Si nos camo con estos 5. Entonces, vuela igual no, tenemos 5 casos y las probabilidades ahora se calculan sobre vuela igual no. A mí feró 2 quintos, habe 1 quinto y 1.2 quinto. Las probabilidades de vuela igual si van a ser 0 para repir un medio para v y un medio para mamíferos. Muy bien. Para cada avance estas probabilidades vuelvo a calcular de este lado la fórmula. Fíjense, bueno, 5 séptimos, esta es la probabilidad del atributo vuela igual no. Esta es la probabilidad del atributo vuela igual si y esta parte de acá es la parte de la sumatoria de la fórmula. Si calculo para cada avance estas probabilidades y sumo tengo la entropía de ese, no sea la entropía de acuerdo, para el atributo A. Para el atributo A con valor igual no. Muy bien. Esto mismo que acabo de hacer, hay que hacerlo para el atributo piel. No, donde puede ser pluma, pelo o camas. Hay que calcular las probabilidades para cada avance estas probabilidades que nos quedan en conjunto de datos y aplicar fíjense que muchos aores acasias en 0 porque pluma, por ejemplo, solo las sabes. Pelo, solo los mamíferos y es camas solo los reptiles. Y interesante. Tumamos y vamos subteniendo, bueno estas son las probabilidades que tenemos acá de cada uno de los antributos y obviamente para el último caso nacimiento. Probablemente igual huevo, Probablemente igual placenta, calculamos, fíjense que de placenta solo los mamíferos y de huevo repiligiaves. Muy bien, una vez que hicimos todos los cálculos y quieren poder repasar la idea positiva más en detalle lo que pasa un poco rápido para ver, no ser la clase tan aburrida. Puede ver que vamos a repasar mejor dicho, vamos a repasar la entropía 1.56. Ahora vemos, atributo igual hueve así, que atributo igual hueve no. La probabilidad de hacer 0.29, se vuelven a que estrogan a un especulado y 0.71 para huele igual no. Y la entropía es 1 para huele igual sí y 1.56 para huele igual no. Si aplicó la formula 1.56 menos la sumatoria de estos dos, ¿todos ves acá? Voy a obtener que la ganancia de información para el atributo A va a ser 0.19. Si repito estos mismos cálculos para huele, para piel y para nacimiento, y obtener que la ganancia de información de cada atributo es 0.01.5 y 0.9.0.85. Que quiere decir esto? Bueno, hay un atributo que me está dando mucha más información que los demás. Y ese atributo es la piel. Si yo veo la piel de la animal, ya puedo decidir de mejor forma que mirando simplemente como nace o si vuelve a uno que tipo de animal es. Entonces voy a empezar a construir mi árboles. No doreais hacer piel. Va a tener tres aristas, uno por cada valor posible, pelo pluma de escama. Y qué es lo que viene acá? Bueno, ahora hay que seguir iterando como vengo por este camino, pelo. Me voy a quedar simplemente con esta parte del dataset. Voy a ver esto que le voy a dar marcado. Pero fíjense que si yo me quedo ya con esta parte del dataset y desestimo el resto, la clase que me queda acá es mamífero mamífero mamífero. Obviamente, si yo hago los cálculos, la probabilidad de que se mamífero va a ser uno. La probabilidad de haber va a ser 0, la probabilidad de haber aquí es 0. Esta parte de la ecuación me va a dar 0. Con lo cual, la entropía de la clase pelo es 0. Hago lo mismo para pluma, la entropía de la clase pluma 0 y la entropía de escama 0. Eso quiere decir que tenemos ya la ropa completo. No tengo que seguir viendo otros atributos. Acá terminó en este ejemplo, lo simple va a pasar. Pelo va a ser mamífero, pluma B y repetí. Estos acá son los nodos terminales. Si yo quiero me da un ejemplo para yo lo pongo en producción de este algoritmo, este árbol, me da un ejemplo nuevo con varias columnas. Bien nacimiento y vuelas. Entre el ordenador Rays veo piel a que es igual a pelo, listo mamífero y así funciona. Bueno, vamos a continuar con la empresa de Shini. Ampazamos un par de días desde que grabé la primera parte del vídeo, pero. Vamos a continuar exactamente desde donde terminó. La parte de entropía. Muy bien, ¿qué es la impureza de Shini? Bueno, hay una definición bastante formal. La verdad es, aunque no es completamente de mi agrado. La empresa de Shini es una medida de cuana menudo un elemento elegido aleatoriamente del conjunto. Sería etiquetado correctamente o sería etiquetado. Si fue etiquetado de manera aleatoria, de acuerdo a la distribución de las etiquetas en el subconjunto, no se entiende nada. Entonces, vamos a hacer un ejemplo y explicarlo que no le haya sido bastante mucho más sencillo que lo que pueden imaginar. Muy bien, ¿por qué vamos a hablar la impureza de Shini? Básicamente, porque la mayoría de las bibliotecas, como Sikerland, utilizan por defecto impureza de Shini y no la entropía. ¿Por qué? ¿Por qué es más fácil de calcular? Es computacionalmente menos costosa y es equivalente. Pero en general, por ejemplo en Sikerland, ustedes cuando entrenan una rol, le pueden dar como un hiperparámetro si quieren entrenarlo con Shini o con la entropía. Muy bien, esta sería la fórmula, la que ven acá. O sea que la fórmula ya ha entrado, es más simple que esta definición. Es uno menos la sumatoria de las diferentes probabilidades de cada elemento en el conjunto C, él va a ser cuadrado. Muy bien. Veamos un ejemplo paso a paso de cómo capturar el impresión de Shini para que estés en un idioma mucho más tan gible de cómo funciona esta medida. Este es mi dataset. Lo que está mirando acá es el dataset. Y el dataset es un dataset que tiene tres columnas, chest pain, que es el horro de pecho, good blood circulation, que significa buena circulación sanguínea y arterias bloqueadas que en ingleses block at trees. Y lo que yo quiero saber, o sea, mi variable dependiente es si hay o no una enfermedad del corazón, un hard disease. Y las posibles son si o no, no, oyes. Muy bien. Entonces voy a construir un árbol, no al igual que en ID3, quiero construir un árbol. De hecho, vamos a usar ID3 pero con otro método. Entonces, chest pain, del horro de pecho, good blood circulation y block at trees, que pasa con estas. Bien, cualquiera de estas tres columnas son candidatas, hacer un odor raíz. Esto quiere decir que si yo agarró una, por ejemplo, la primera, chest pain y la uso como no de raíz, y pongo dos salidas porque tiene dos valores, plugo false y directamente pongo debajo de una de las flechas, hard disease y es, y no, de hard disease no. Ya tendría como mi primer odor raíz, pero lo que yo quiero ver ahora es que también, que también está clasificando esta columna. Es como si, imagínem, un segundo, es como si yo quisiera hacer un árbol o un clasificador, pero con un solo valor. Puedo usar chest pain o puedo usar un blood circulation o puedo usar block at trees, block at trees, cual sería el que menos errores comenta. Bueno, entonces agarró el primero, chest pain digo true of false, no, y eso no, y de este lado voy a ver cuántas salen por yes y de este lado, cuántas salen por no y lo mismo para heresis. Entonces, tomo la primer pila, block at trees digo, perdón, me digo que chest pain, no, es decir false, todo algo por acá. Y hard disease no, entonces computo un valor acá, no, sí, muy bien, sigo con la siguiente columna. Es yes, salgo por a este lado true y hard disease es yes, entonces pongo un, uno acá, contador en este lado, muy bien. Sigo con este de acá, sigo con este de acá, que es yo, hasta que complete el data set, cuando completé el data set me va a quedar algo como lo que ven acá. Cuando chest pain es true, tengo 105 casos de hard disease y tuv19 de no. Y cuando chest pain es false, tengo 34 casos de hard disease y 125 de no. No, hard disease, o sea, no hay informe del corazón, ¿qué quiere decir esto? Que en principio si yo soy chest pain podría decir que si hay dolor de pecho, la informe del corazón, digamos es más probable porque en el caso de 10, creo que si no hay la enfermedad del corazón es menos probable porque mayor casos de no. Tiene embargo que uno clasifica perfectamente si yo lo tomás así, comete algunos errores porque hay algunos casos de 10 acá y algunos casos de no en este lado. Y bueno, hago esto mismo para la siguiente columna, God the Terculation, que también puede ser no, hoy es otro fall. Y con puntos, para cada uno de estos casos, hay buena circulación sanguínea, hay informe de la corazón, si, 37 casos no, suyendo 27 y lo mismo para el otro lado. Finalmente repito la misma operación con la última de las columnas, block of the earth, la tería bloqueada, el terío volcada así, bueno, la enfermedad de corazón puede ser 92 casos o 31 o que no. Y 100 falls pueden ser casos positivos, 45 y negativo, 129. Muy bien, entonces ya esto entraba estando en unidad de que hay una de estas columnas que clasifican mejor o que comete menos errores que las demás. No, muy bien, esto lo que les digo no clasifica bien el 100 por ciento de los casos, pero cuál es mejor. Bien, en este ejemplo van a poder ver que el total si no siempre coincide, porque se supone que este me lo también contempla valores nulos, faltantes y eso. Faltantes y esos valores no los computamos, se omiten, eso quiere decir que si yo sumo todos estos casos acá y sumo todos estos casos acá y sumo todos estos casos acá. Quizás no haya, o sea, la misma cantidad para ahí en el dataset tengo mil registros, pero bueno, algunos valores para que se pensan nulos, otros para que los circulaciones son nulos, esos no los pongo, no los computo para calcular la empresa de Shin. Muy bien, entonces ninguno de estos nodos que vemos acá es un 100 por ciento, y es hay una enfermedad de corazón o no, no hay un hardisismo, hay una enfermedad de corazón. Eso quiere decir que ninguno es puro, ninguno de estos nodos es un nodo, puro, son los impuros. Entonces para saber cuál clasifica mejor tenemos que medir la impureza de cada nodo y de cada árbol y hay varias formas de medir esto, una forma, una es de impureza de Shin, que es muy popular. Entonces como la medimos, muy bien, acá tenemos la formula, sería uno menos la probabilidad de yes al cuadrado menos la probabilidad no al cuadrado y eso me da la impureza de este nodo. Y como sería eso, uno menos la probabilidad de yes es 105 sobre 105 más 39, todo el cuadrado menos la probabilidad de no cuándo es 39 sobre 15 más 39 también al cuadrado y eso me da 0.395 y esa es la impureza de Shin calculada para este nodo. Tenía que armeer, fijado que estaba todo eso. Muy bien, ¿qué pasa con el siguiente nodo? Bueno, también hacemos de más cuenta y vemos que da 0.336. Entonces la impureza de Shin, ahora es para este nodo 395 y para este 0.336. Muy bien. Ahora lo que queremos calcular es la impureza de Shin para el total, o sea para chess pen y para el nodo raíz. Y eso es un promedio ponderado. Como se ponderá, bueno, fíjense las cantidades que tenemos en cada nodo, este tiene 144 casos y este 159 casos. Entonces la impureza de Shin y venden ya a sal a la siguiente. Estas recuerdan, estos lo que hagamos calculado antes de cada hoja y este sería el promedio ponderado. 144 sobre 144 más 159, que como pueden ver son los valores que habíamos dicho que de totales cabían cada nodo y en este caso 159 sobre 144 más 159. Entonces esta promedio ponderado es 0.364 y ese es la impureza de Shin para el nodo chess pen. Obviamente esta buena zona tenemos que hacer para los 3 nodos que calculen sus 3 arbolitos que calculen acá y voy a ver que la impureza de Shin y para chess pen es 0.324, como vimos recién para Good Blood Circulation es 0.360 y para Blocked Arteries es 0.381 y esto que quiere decir que este de acá tiene la menor impureza de Shin y la más chica con lo cual este nodo Good Blood Circulation tiene mejores chances para clasificar si lo colocamos con un nodo raíz. Y esta es una forma alternativa de construir un árbol y regular usar la entropía y la gananza de información usamos la impureza de Shin. Pero el proceso de costumbre el árbol es exactamente el mismo, no cambia nada. Muy bien, T4.5, que es T4.5, T4.5 es una mejora de la gorinmo de 3. En realidad hoy en día cada vez que estemos trabajando con un árbol de decision, vamos a estar trabajando con una R4.5 o mejoras sobre este árbol. Y el árbol y de 3 en realidad ya no se usa porque fue superado. Este algoritmo fue también bastante pionero, o sea, no es nuevo y que mejoras tiene respecto de y de 3. Primero que soporta campos numéricos y rangos continuos, cosa que el tira 3 no lo soportaba, el tira 3 solo se portaba variables de tipo cualitativas que tomaron un valor de una serie de valores posibles. Contemplar también la posibilidad de usar datos saltantes, o sea que el árbol y de 3 no soportaba y se agregó un método alicional de poda que lo que hace es evitar evitar el overfitting. Que es algo que habíamos ya visto en la primera clase, muy bien. Datos altantes, muy bien, esto es muy sencillo. M acá el manejo de los datos faltantes se hace como lo haría cualquiera de ustedes estuvimos simplemente en el algoritmo. ¿Qué significa? Pásicamente los atributos faltan de los marca con un signo o el trogasión y simplemente no se usan en los cálculos de la ganancia y la entropía. O como vimos recién en la ganancia de Shinny tampoco los omitimos en la cuenta. Muy bien, esto es un poco más complejo como hace para manejar campos numéricos, rango continuos porque un árbol en principio tiene una rama para cada valor posible. Una rama para así o para no, o por ejemplo si fuera temperatura baja media alta tendría tres ramas, tres salistas de salida. Pero tuve que cuando la temperatura en realidad en vez de ser baja media alta es un número que está en un rango continuo que puede ir, no sé, de menos 20 grados, a 40 grados con un montón de variaciones en el medio. No podemos poner ahí, no sé, 200 ramas de salida por cada número posible. Entonces lo que se hace es dar una tributua que sea un rango continuo y el algoritmo dinámicamente va a crear un campo buliano, se va a convertir al rango continuo en una nueva columna de tipo buliana trufol. Y eso lo va a hacer a partir de un humbral, un humbral c, va a encontrar un valor, supongan ser de nuevo, ejemplo de la temperatura, un humbral c que sea ser logrado por imente ser algo, tal que si el valor es menor que ser logrado trufo, mayor igual que ser logrado, false, listo. Entonces este humbral c lo va a obtener buscando aquel que déis la mayor ganancia de información. Muy bien, ¿cómo se hace? Primero se ordena esta columna de menor a mayor, entonces su afuera de temperatura se empezamos en la temperatura más baja, a menor hasta la más alta. Luego identificamos los valores atyacentes de la clase que es nuestra salida, ¿cuál sería la salida? No sé qué podría estar queriendo decir, pero por ejemplo si debería o no llevar para aguas, podría o no llevar campera. Bueno, lo que hace es ordena de menor a mayor y luego va chequeando por cada registro que es lo que dice la clase salida. Y cuando noto una variación, por ejemplo dice sin campera, sin campera, sin campera con campera, ese, ok, acá hay un cambio, para esta temperatura veo un cambio y entonces lo marca como un seguí, es decir, como un límite candidato. Y así queda varios, o sea, que van a dividir ese conjunto en varios rangos. Luego para acá unos dos rangos calcula la ganancia de información y obviamente se queda con aquel que le provea el mejor resultado. Hay variaciones de algoritmo donde se pueden también quedar con N mejores, si no es de base a los guillanos tipo Truffold, bueno se pueden quedar con tres, cuatro dependiendo del implementación del algoritmo. Por último, la mejora adicional del algoritmo, si 4.5 es la poda. Como vimos, estos árboles para determinados problemas pueden generar Oberfit y, es decir, pueden ajustar muy bien, pero el conjunto de entrenamiento y no generalizar para datos nuevos, como por ejemplo los que va a ver en el conjunto de test. Entonces lo que queremos es meter algún mecanismo tal que si se está ajustando muy bien a los datos de training, no lo haga, no lo haga. Muy bien, entonces el método de poda consiste en lo siguiente, uno generamos el árbol, como vimos hasta ahora. Luego vamos a analizar de forma recursiva y desde las hojas que preguntas o no los interiores se pueden eliminar sin que simplemente el error de clasificación con el conjunto de test. Por ejemplo, se ha erruido el árbol va a tener cierto error, una cantidad de casos más clasificados. Y el error bueno es básicamente los casos bien clasificados sobre casos totales, pero esta es una medida podría usar cualquier otra como las que vimos en la clase anterior. Entonces, si se ha ido en la diapo adicional, si se elimina uno de interior, cuyos sucesores son todos los dos hojas, es decir, empezamos a recorrer el árbol, pero no de la raíz, es decir, de la parte de arriba. Si no de la parte de abajo, los no dos hoja. De la parte de abajo, muy bien. Subimos un nivel, encontramos un nodo pregunta, un nodo que tiene abajo solo no dos terminales y lo eliminamos. Y volvemos a calcular el error que cometemos en el conjunto de test. ¿Qué pasó con el error? Mejoró, o sea, al sacar ese nodo, el conjunto de evaluación, el conjunto de test es mejor, lo dejamos borrado. Si no, ese nodo lo dejamos. Y este proceso se repiten. Si vuelve a pasar de nuevo, esta vez atacará al nodo padre, de ese nodo, porque ahora ese nodo se acuerdió en un terminal. Si no lo borra, seguirá con los no dos hermanos. Y esa es la forma en la cual procede este mecanismo de poda. Muy bien. El último tema que vamos a ver el aclacido es random forest. Random forest no es exactamente un árbol. Algunos están sufichando que es un bosque. Es un ensamble. Todavía no vamos a ver ensamble, se vamos a ver en otra clase. Pero este ensamble en particular es un ensamble criado con todos árboles. Así que voy a explicarlo. Voy a explicar sus nuevementos desde el punto de vista de los árboles y más adelante cuando veamos mecanismos de ensambles, vamos a entender mejor cómo funciona random forest. La idea de atrás de random forest es la siguiente. Muchos estimadores, mediocres, pero mediados pueden ser muy buenos estimadores. Vamos a volver de nuevo a esta idea. Bustrap Aggregating. Este es el nombre de una técnica. Ya vamos técnica o meta algoritmos porque meta algoritmos porque es una técnica de los propietarios generados de algoritmos. Está como arriba de algoritmos. Está de un conjunto de entrenamiento de tamaño N, esta técnica de baje, lo que va a hacer es generar nuevos conjuntos. Va a agarrar ese conjunto, va a generar conozco M conjuntos, que van a hacer un muestreo del conjunto original y lo va a llamar de su uno, de su dos, etc. Cada uno va a tener N prima, o sea, si el primero tenía mil, esto va a tener, no sé, 600. Básicamente, N prima se calcula como dos tercios de N, más o menos puede variar. La primera idea es tenemos un dataset, el dataset es muy grande y este es uno de los primeros problemas que tiene los árboles. Arboles nos oportan ser entrenados con 100.000 de millones de registros, o sea, no, soportan el big data. Empiezan a fallar, digamos, no dan buenas resultados. Entonces, ¿qué se puede hacer? Bueno, primero ese conjunto tan grande partir, hacer conjuntos más chiquitos, entrenamiento. Entonces, por ejemplo, tenemos 8 adributos, una clase que queremos saber o sea una variable dependiente y 9.000 registros. Yo aplico esta técnica de bus strap agregating y ahora genero M, conjuntos, M, subtablas, que van a tener cada una, solo 6000 registros. Vamos a agregar también esta técnica una variante que es el attribute bag in or random subspace que no siempre se usa con esta técnica, que es para eliminar también columnas. Así como eliminamos filas, vamos a quitar columnas de cada uno de estos subconjuntos. Con cuanto a nos quedamos bueno, que la raíz cuadrada del número de adributos. Si hay 8 adributos, tomamos raíz cuadrada de 8, 3, es decir, que éramos estos subconjuntos, solo con 3 columnas. Obviamente esta técnica es la que cuenta de algo cuando hay muchas columnas, cuando hay 8 pasaradas, pero si hay muchas columnas, simplemente esto también. Muy bien. Entonces vamos a tener ahora en metablas cada una reducción hace en el tributo. Para cada una de ellas entrenamos un árbol. Para cada árbol creamos la matriz de confusión, que es algo que vimos en la clase de métricas. Entonces vamos a saber básicamente, cuáles son los trupositive, tiene, los trunegatip, los false negative y los false positive. Y vamos a poder ver el desempeño de cada uno de estos árboles. Puedes ver el siguiente paso. Calculamos la tasa de error de cada árbol, que se calcula como los falsos positivos, falsos negativos, sobre el total de ejemplos clasificados. Y acá está una aclaración, esto se hace sobre un conjunto de entrenamiento o en algunos casos sobre un percentage del conjunto creado. Nos quedamos, obviamente, con el mejor árboles de esos emes, o sea, si teníamos emes conjuntos, vamos a tener emes árboles para cada uno de estos emes árboles, porque tenemos esta tasa de error y aquel que tenga una tasa de error más chica, bueno, nos lo quedamos. Y el otro, los otros los descartamos. Este proceso, lo repetimos, cabeces, es decir, sobre cabeces, vamos a generar emes conjuntos, formaliatoria, vamos a entrenar árboles para acá con esos conjuntos y vamos a quedar con el mejor. Entonces, al final de este proceso, vamos a tener ca árboles. Cada uno de esos árboles, que en todos otros entrenamos, toscar árboles, va a ser un árbol muy malo, porque no está entrenado sobre la tonetalía del conjunto ni siquiera, sobre la tonetalía de las columnas, con lo cual, cuando yo lo ponga en producción a predecir valores del conjunto real y no va a ser muy bueno. No puede ser muy bueno. Es un clasificador medio crezpeiro, pero muchos clasificadores medio crezcan como resultado un buen clasificador. Entonces, para poder ver el resultado final de random forest, voy a hacerlo siguiente. Voy a hacer que cada uno bote. Entonces, entra un nuevo caso, una vez que ya tengo mi random forest creado. El árbol 1 me dice, es clase 1, el árbol 2 me dice, es clase 2, el árbol 3 me dice, es clase 1, y así, cada uno bota. Y lo que dijo la mayoría, es el resultado final. Lo que dijo la mayoría es básicamente el resultado del clasificador. Entonces, random forest, básicamente, como su nombre indica, es un bosque de árboles creado de manera leatoria. Cada uno de ellos es un estimador medio cre, pero todos en conjunto son un excelente clasificador. Bueno, eso fue todo por la clase Neoi, la clase de árboles, espero verlos pronto.